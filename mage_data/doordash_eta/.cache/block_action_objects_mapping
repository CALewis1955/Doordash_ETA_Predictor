{"block_file": {"custom/save_evidently_report_to_s3.py:custom:python:save evidently report to s3": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom os import path\nfrom datetime import datetime\nimport os\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\n\n@custom\ndef transform_custom(data):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n    AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n    AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n    AWS_DEFAULT_REGION = os.environ['AWS_DEFAULT_REGION']\n    \n    \n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.html'   \n    \n    bucket_name = 'mlflow-clewis916-remote'\n    object_key = f'evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}'\n    with open(filepath, 'r', encoding='utf-8') as file:\n        evidently_report = file.read()\n\n    # Initialize the S3 client\n    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)                         \n    \n    try:\n    # Upload the file to S3\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=evidently_report)\n        print(f\"File uploaded successfully to {bucket_name}/{object_key}\")\n    except FileNotFoundError:\n        print(\"The file was not found\")\n    except NoCredentialsError:\n        print(\"Authentication failed\")\n    except Exception as e:  \n        print(f\"An error occurred: {e}\")\n    \n    print(f'This is the type: {type(evidently_report)}')\n    return evidently_report\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/save_evidently_report_to_s3.py", "language": "python", "type": "custom", "uuid": "save_evidently_report_to_s3"}, "custom/evidently_report.py:custom:python:evidently report": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/evidently_report.py", "language": "python", "type": "custom", "uuid": "evidently_report"}, "data_exporters/save_report_to_s3.py:data_exporter:python:save report to s3": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom pandas import DataFrame\nfrom os import path\nfrom datetime import datetime\nimport os\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_report_to_s3(html, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a S3 bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#s3\n    \"\"\"\n    AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n    AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n    AWS_DEFAULT_REGION = os.environ['AWS_DEFAULT_REGION']\n    \n    \n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.html'   \n    \n    bucket_name = 'mlflow-clewis916-remote'\n    object_key = f'evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}'\n    with open(filepath, 'r', encoding='utf-8') as file:\n        evidently_report = file.read()\n\n    # Initialize the S3 client\n    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)                         \n    \n    try:\n    # Upload the file to S3\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=evidently_report, ContentType='text/html')\n        print(f\"File uploaded successfully to {bucket_name}/{object_key}\")\n    except FileNotFoundError:\n        print(\"The file was not found\")\n    except NoCredentialsError:\n        print(\"Authentication failed\")\n    \n    return evidently_report", "file_path": "data_exporters/save_report_to_s3.py", "language": "python", "type": "data_exporter", "uuid": "save_report_to_s3"}, "data_loaders/load_data.py:data_loader:python:load data": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n    df = pd.read_csv('historical_data.csv')\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_data.py", "language": "python", "type": "data_loader", "uuid": "load_data"}, "transformers/get_rf_prediction.py:transformer:python:get rf prediction": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport pickle\nimport mlflow\nimport os\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\nAWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\nAWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\nAWS_DEFAULT_REGION =os.environ['AWS_DEFAULT_REGION']\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    df = data\n\n    numerical = ['max_item_price', 'min_item_price', 'subtotal', 'total_items', 'num_distinct_items', 'max_item_price', 'min_item_price', 'total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n\n    categorical = ['market_id', 'store_id', 'store_primary_category', 'order_protocol']\n\n    \"\"\"## Validation Framework\"\"\"\n\n\n    df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n    df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\n    print(f'This is the length of the training, validation, and test sets: {len(df_train)}, \\\n        {len(df_val)}, {len(df_test)}')\n\n    df_train = df_train.reset_index(drop=True)\n    df_val = df_val.reset_index(drop=True)\n    df_test = df_test.reset_index(drop=True)\n\n    y_train = df_train.actual_duration.values\n    y_val = df_val.actual_duration.values\n    y_test = df_test.actual_duration.values\n\n    train_dict = df_train[categorical + numerical].to_dict(orient='records')\n    val_dict = df_val[categorical + numerical].to_dict(orient='records')\n\n    params = dict(max_depth=20, n_estimators=100, min_samples_leaf=10, random_state=0)\n\n    pipeline = make_pipeline(\n        DictVectorizer(),\n        RandomForestRegressor(**params, n_jobs=-1)\n    )\n      \n    now = datetime.now()\n    \n    mlflow.set_tracking_uri(uri=\"http://ec2-34-233-122-168.compute-1.amazonaws.com:5000\")\n    mlflow.end_run()\n    mlflow.set_experiment(f'RF-{now}')\n    mlflow.set_tag(\"model\", \"random forest regressor\")\n    mlflow.autolog()\n      \n    pipeline.fit(train_dict, y_train)\n    y_pred = pipeline.predict(val_dict)\n\n    rmse = mean_squared_error(y_pred, y_val, squared=False)\n    print(f'This is the rmse:  {rmse}')\n    mlflow.sklearn.log_model(pipeline, artifact_path=\"model\")\n \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n", "file_path": "transformers/get_rf_prediction.py", "language": "python", "type": "transformer", "uuid": "get_rf_prediction"}, "transformers/get_prediction.py:transformer:python:get prediction": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport pickle\nimport mlflow\nimport os\nimport joblib\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\nAWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\nAWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\nAWS_DEFAULT_REGION =os.environ['AWS_DEFAULT_REGION']\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    df = data\n\n    numerical = ['max_item_price', 'min_item_price', 'subtotal', 'total_items', 'num_distinct_items', 'max_item_price', 'min_item_price', 'total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n\n    categorical = ['market_id', 'store_id', 'store_primary_category', 'order_protocol']\n\n    \"\"\"## Validation Framework\"\"\"\n\n\n    df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n    df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\n    len(df_train), len(df_val), len(df_test)\n\n    df_train = df_train.reset_index(drop=True)\n    df_val = df_val.reset_index(drop=True)\n    df_test = df_test.reset_index(drop=True)\n\n    y_train = df_train.actual_duration.values\n    y_val = df_val.actual_duration.values\n    y_test = df_test.actual_duration.values\n\n    train_dict = df_train[categorical + numerical].to_dict(orient='records')\n    val_dict = df_val[categorical + numerical].to_dict(orient='records')\n\n\n    pipeline = make_pipeline(\n        DictVectorizer(),\n        LinearRegression()\n    )\n      \n    now = datetime.now()\n    \n    mlflow.set_tracking_uri(uri=\"http://ec2-34-233-122-168.compute-1.amazonaws.com:5000\")\n    mlflow.end_run()\n    mlflow.set_experiment(f'LR-{now}')\n    mlflow.set_tag(\"model\", \"linear regression\")\n    mlflow.autolog()\n      \n    pipeline.fit(train_dict, y_train)\n    y_pred = pipeline.predict(val_dict)\n\n    rmse = mean_squared_error(y_pred, y_val, squared=False)\n    print(f'This is the rmse:  {rmse}')\n    mlflow.sklearn.log_model(pipeline, artifact_path=\"model\")\n\n    joblib.dump(pipeline, './mage_data/pipeline.pkl')\n    print(\"Pipeline saved successfully.\")\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n", "file_path": "transformers/get_prediction.py", "language": "python", "type": "transformer", "uuid": "get_prediction"}, "transformers/write_evidently_to_influxdb.py:transformer:python:write evidently to influxdb": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom influxdb import InfluxDBClient\nimport json\nfrom datetime import datetime\n\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.py'\n    \n    client = InfluxDBClient(url='localhost:8086', username='user', password='password')\n    client.switch_database('evidently_metrics')\n    # Load your Evidently report\n    with open(filepath, 'r') as f_in:\n        evidently_report = json.load(f_in) \n    # Prepare InfluxDB data\n    influx_data = [\n        {\n            \"measurement\": \"model_metrics\",\n            \"tags\": {\n                \"model\": \"your_model_name\",\n            },\n            \"fields\": evidently_report  # Ensure report_data is a flat dictionary\n        }\n    ]\n    # Write data to InfluxDB\n    client.write_points(influx_data)\n    return data\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/write_evidently_to_influxdb.py", "language": "python", "type": "transformer", "uuid": "write_evidently_to_influxdb"}, "transformers/clean_data.py:transformer:python:clean data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    \n\n    df = data\n    # Loaded variable 'df' from URI: /home/ubuntu/mage/data/historical_data.csv\n\n    # Drop rows with missing data if count of missing rows is less than 3% of all rows\n    # in columns: 'market_id', 'actual_delivery_time' and 3 other columns\n    columns_with_insubstantial_missing_data = [col for col in df.columns if df[col].isnull().sum() < 0.03 * len(df)]\n    # market_id', 'actual_delivery_time', 'store_primary_category', 'order_protocol', 'estimated_store_to_consumer_driving_duration'\n    df.dropna(subset=columns_with_insubstantial_missing_data, inplace=True)\n\n    # if more than 3%, replace missing values with mean\n    columns_with_substantial_missing_data = [col for col in df.columns if df[col].isnull().sum() >= 0.03 * len(df)] \n    for column in columns_with_substantial_missing_data:\n        df = df.fillna({column: df[column].mean()})\n\n    \n    return df\n    \n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    \n    assert output is not None, 'The output is undefined'\n\n", "file_path": "transformers/clean_data.py", "language": "python", "type": "transformer", "uuid": "clean_data"}, "transformers/make_fake_data.py:transformer:python:make fake data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport random\nimport datetime\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    df = data.copy(deep=True)\n    # Function to vary numerical data by a random percentage up to 3%\n    dollar_values = ['max_item_price', 'min_item_price', 'subtotal', 'max_item_price', 'min_item_price']\n    integer_values = ['total_items', 'num_distinct_items','total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n    \n    def fake_dollar_data(value):\n        percentage = random.uniform(-0.10, 0.10)  # Random percentage between -3% and 3%\n        return round((value * (1 + percentage)), 2)\n    \n    def fake_integer_data(value):\n        number = random.randint(0,4)\n        return value + number\n    \n    \"\"\"\n    def fake_datetimes(value):\n        time =pd.to_datetime(value)\n        time = (time.hour * 3600 + time.minute * 60 + time.second) / 10**9\n        number = random.randint(0, 10**6)\n        return time + number\n    \"\"\"\n    \n    # Apply the function to each numerical column in the dataframe\n    for column in df[dollar_values]:\n        df[column] = df[column].apply(fake_dollar_data)\n    for column in df[integer_values]:\n        df[column] = df[column].apply(fake_integer_data)\n    \"\"\"    \n    df['created_at'] = pd.to_datetime(df['created_at']).astype(int) / 10**9\n    df['actual_delivery_time'] = df['actual_delivery_time'].apply(fake_datetimes)\n    \"\"\"\n    filename = f'./mage_data/fake_data_{datetime.datetime.now().strftime(\"%m-%d-%Y\")}.csv'  \n    df.to_csv(filename, index=False)   \n    return data, df\n\n    \n    \n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/make_fake_data.py", "language": "python", "type": "transformer", "uuid": "make_fake_data"}, "transformers/evidently_report.py:transformer:python:evidently report": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metrics import ColumnDriftMetric, DatasetDriftMetric, DatasetMissingValuesMetric\nfrom datetime import datetime\nimport mlflow\nimport os\nimport json\nimport joblib\n\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    def add_target_df(df):\n        df['created_at'] = pd.to_datetime(df['created_at']).astype(int) / 10**9\n        df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time']).astype(int) / 10**9\n        df['actual_duration'] = (df['actual_delivery_time'] - df['created_at']) / 60\n        return df\n\n    numerical = ['max_item_price', 'min_item_price', 'subtotal', 'total_items', 'num_distinct_items', 'max_item_price', 'min_item_price', 'total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n    categorical = ['market_id', 'store_id', 'store_primary_category', 'order_protocol']\n\n    reference_data = add_target_df(data[0]) #original data \n    reference_dict = reference_data[categorical + numerical].to_dict(orient='records')\n\n    \n    current_data = add_target_df(data[1]) #dummy data\n    current_dict = current_data[categorical + numerical].to_dict(orient='records')\n\n\n    # retrieve model \n    pipeline = joblib.load('./mage_data/pipeline.pkl')\n\n    reference_preds = pipeline.predict(reference_dict)\n    reference_data['prediction'] = reference_preds\n\n    current_preds = pipeline.predict(current_dict)\n    current_data['prediction'] = current_preds\n\n    column_mapping = ColumnMapping(\n        target=None,\n        prediction='prediction',\n        numerical_features=numerical,\n        categorical_features=categorical\n    )\n\n    report = Report(metrics=[\n        ColumnDriftMetric(column_name='prediction'),\n        DatasetDriftMetric(),\n        DatasetMissingValuesMetric()\n    ]\n    )\n\n    report.run(reference_data=reference_data, current_data=current_data, column_mapping=column_mapping)\n\n    report.show()\n\n    evidently_report = report.as_dict()\n\n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.py'\n    with open(filepath, 'w') as f_out:\n        json.dump(evidently_report, f_out, indent=4)    \n    \n    print(f\"Evidently report has been stored in JSON format to {filepath}\")\n    print(f\"This is the type: {type(evidently_report)}\")\n    return evidently_report\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/evidently_report.py", "language": "python", "type": "transformer", "uuid": "evidently_report"}, "transformers/add_target_to_df.py:transformer:python:add target to df": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    \n    \"\"\"\n    Returns a dataframe with the duration of the delivery in minutes\n    \"\"\"\n    df = data\n    # our target is actual duration\n    # the created_at and actual_delivery_time are dates;  let's convert them\n    df['created_at'] = pd.to_datetime(df['created_at']).astype(int) / 10**9\n    df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time']).astype(int) / 10**9\n    df['actual_duration'] = (df['actual_delivery_time'] - df['created_at']) / 60\n    return df\n    \n    \n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n", "file_path": "transformers/add_target_to_df.py", "language": "python", "type": "transformer", "uuid": "add_target_to_df"}, "/home/src/doordash_eta/transformers/evidently_report.py:transformer:python:home/src/doordash eta/transformers/evidently report": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metrics import ColumnDriftMetric, DatasetDriftMetric, DatasetMissingValuesMetric\nfrom datetime import datetime\nimport mlflow\nimport os\nimport json\nimport joblib\n\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    def add_target_df(df):\n        df['created_at'] = pd.to_datetime(df['created_at']).astype(int) / 10**9\n        df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time']).astype(int) / 10**9\n        df['actual_duration'] = (df['actual_delivery_time'] - df['created_at']) / 60\n        return df\n\n    numerical = ['max_item_price', 'min_item_price', 'subtotal', 'total_items', 'num_distinct_items', 'max_item_price', 'min_item_price', 'total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n    categorical = ['market_id', 'store_id', 'store_primary_category', 'order_protocol']\n\n    reference_data = add_target_df(data[0]) #original data \n    reference_dict = reference_data[categorical + numerical].to_dict(orient='records')\n\n    \n    current_data = add_target_df(data[1]) #dummy data\n    current_dict = current_data[categorical + numerical].to_dict(orient='records')\n\n\n    # retrieve model \n    pipeline = joblib.load('./mage_data/pipeline.pkl')\n\n    reference_preds = pipeline.predict(reference_dict)\n    reference_data['prediction'] = reference_preds\n\n    current_preds = pipeline.predict(current_dict)\n    current_data['prediction'] = current_preds\n\n    column_mapping = ColumnMapping(\n        target=None,\n        prediction='prediction',\n        numerical_features=numerical,\n        categorical_features=categorical\n    )\n\n    report = Report(metrics=[\n        ColumnDriftMetric(column_name='prediction'),\n        DatasetDriftMetric(),\n        DatasetMissingValuesMetric()\n    ]\n    )\n\n    report.run(reference_data=reference_data, current_data=current_data, column_mapping=column_mapping)\n\n    report.show()\n\n    # saving report in html format\n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.html'\n    report.save_html(filepath)\n    print(f\"Evidently report has been stored in html format to {filepath}\")\n\n\n    evidently_report = report.as_dict()\n    filepath = f'./mage_data/evidently_report_{datetime.now().strftime(\"%m-%d-%Y\")}.py'\n    with open(filepath, 'w') as f_out:\n        json.dump(evidently_report, f_out, indent=4)  \n    print(f\"Evidently report has been stored in JSON format to {filepath}\")\n    \n    return evidently_report\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/doordash_eta/transformers/evidently_report.py", "language": "python", "type": "transformer", "uuid": "evidently_report"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}